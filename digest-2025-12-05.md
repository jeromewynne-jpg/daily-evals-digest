# AI Evaluation Papers Digest - 2025-12-05

## Table of Contents
- [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](#sea-safeguardbench-evaluating-ai-safety-in-sea-languages-and-cultures)
- [Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice](#simulating-life-paths-with-digital-twins-ai-generated-future-selves-influence-decision-making-and-expand-human-choice)
- [Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity](#future-you-designing-and-evaluating-multimodal-ai-generated-digital-twins-for-strengthening-future-self-continuity)

---

## [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501v1)

**Authors & Affiliations**: Panuthep Tasawong (VISTEC), Jian Gang Ngui (AI Singapore), Alham Fikri Aji (Google), Trevor Cohn (Google), Peerat Limkonchotiwat (AI Singapore)

**Models Tested**: OpenAI: GPT-4o, GPT-4.1, GPT-4.1-mini, GPT-OSS 20B. Google: Gemma-2-it 9B, Gemma-3-it 4B/27B, Gemini-2.0/2.5 flash, Google Model Armor. Meta: Llama-3.1-it 8B/70B, Llama-3.2-it 3B, Llama-3.3-it 70B, LlamaGuard-3 1B/8B, LlamaGuard-4 12B. Also tested: ShieldGemma 2B/9B, PolyGuard variants, LionGuard-2, X-Guard, Azure AI Content Safety, OpenAI Moderation, LakeraGuard, Gemma-SEA-LION variants

**Research Question**: How well do current LLM safeguards perform on Southeast Asian languages and culturally-specific safety scenarios, and do they maintain equivalent safety standards across languages compared to English?

**Claim**: The paper demonstrates that state-of-the-art LLMs and safeguard models significantly underperform on Southeast Asian languages and cultural contexts compared to English, revealing critical gaps in multilingual and culturally-aware safety alignment that cannot be addressed by simple translation approaches.

**Method**: The authors created SEA-SafeguardBench with three subsets: (1) General subset with 4,800 human-verified translations of existing English safety datasets across 8 languages, (2) In-the-wild subset with 6,020 natively-authored culturally-specific prompts by native speakers, and (3) Content generation subset with 3,010 prompts targeting culturally inappropriate content generation. All samples were human-verified by native speakers passing English proficiency tests, with culturally-grounded topics covering 1,338 unique cultural topics including local norms, taboos, and region-specific sensitivities.

**Results**: Safeguard models show average AUPRC drops of 5.7 (general), 6.1 (ITW-cultural), and 5.4 (CG-cultural) points on SEA languages vs. English for prompt classification. Tamil and Burmese are most challenging. Performance degrades substantially on culturally-grounded content (36.4 AUPRC drop for English, 36.2 for SEA on CG subset). Response classification is harder than prompt classification across all models. Zero-shot models sometimes outperform fine-tuned models on cultural subsets. Models fail to express calibrated uncertainty on sensitive/ambiguous content.

**Limitations**: The benchmark covers only 7 SEA countries (excludes Brunei, Laos, Cambodia, East Timor due to annotator availability). All annotators were university students who passed English proficiency tests, which may not represent broader population perspectives. The 'sensitive' label shows lower annotator agreement, indicating inherent difficulty in defining cultural harm boundaries. The study uses fixed 0.5 classification thresholds for some analyses despite showing this is often suboptimal. Cultural awareness prompting experiments assume oracle knowledge of target culture, which may not be realistic in deployment.
## [Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice](https://arxiv.org/abs/2512.05397v2)

**Authors & Affiliations**: Rachel Poonsiriwong (MIT Media Lab), Chayapatr Archiwaranguprok (MIT Media Lab), Constanze Albrecht (MIT Media Lab), Peggy Yin (Stanford University), Nattavudh Powdthavee (Nanyang Technological University), Hal Hershfield (UCLA), Monchai Lertsutthiwong (KASIKORN Labs), Kavin Winson (KASIKORN Labs), Pat Pataranutaporn (MIT Media Lab)

**Models Tested**: Claude Sonnet 4.5 (for generating future memories and conversational AI), ElevenLabs Voice Model v2 (for voice cloning), Google's Nano Banana (for age progression), LivePortrait (for animation)

**Research Question**: Can AI-generated future self avatars that simulate different life paths influence decision-making in consequential life choices, and how does the configuration of options (single-sided, balanced, or expanded with novel alternatives) affect decision outcomes?

**Claim**: AI-generated future self avatars can measurably influence life decisions, with single-sided presentations producing directional persuasion, balanced presentations increasing overall decision engagement, and system-generated third options expanding choice architecture by surfacing previously unconsidered alternatives at 7x higher adoption rates than control.

**Method**: Randomized controlled trial (N=192) with young adults aged 18-28 facing life decisions. Participants were assigned to control (guided imagination) or one of four avatar conditions using multimodal AI (facial age progression, voice cloning, LLM dialogue). The system generated personalized avatars representing future selves 30 years forward across three dimensions: evaluative, affective, and eudaimonic vividness. Decision outcomes, psychological measures, and vividness ratings were collected pre- and post-intervention.

**Results**: Single-sided avatar exposure approximately doubled decision shift rates toward the presented option compared to control (18.4% vs 8.1% for Option A, 15.4% vs 5.3% for Option B). Balanced dual-avatar presentation significantly increased overall decision movement (34.2% vs 10.8% in control, p=0.015). System-generated Option C achieved 20.0% adoption versus 2.7% in control (p=0.019), a seven-fold increase. Participants valued evaluative reasoning and eudaimonic meaning-making significantly more than affective resonance or visual realism. Perceived persuasiveness and baseline agency predicted decision change.

**Limitations**: Brief intervention duration (7-10 minutes) may not reflect sustained engagement effects. Sample limited to young adults (18-28) in the United States, limiting generalizability. Study assessed decision intentions rather than implemented behaviors, lacking follow-up on actual decision implementation. LLMs can hallucinate details and generate narratives that may poorly reflect actual life outcomes. The Option C generation relied on a single prompting strategy. No examination of potential negative effects such as distress, unrealistic expectations, or identity confusion.
## [Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity](https://arxiv.org/abs/2512.06106v1)

**Authors & Affiliations**: Constanze Albrecht (MIT Media Lab), Chayapatr Archiwaranguprok (MIT Media Lab), Rachel Poonsiriwong (Harvard University), Awu Chen (MIT Media Lab), Peggy Yin (Stanford University), Monchai Lertsutthiwong (KASIKORN Labs), Kavin Winson (KASIKORN Labs), Hal Hershfield (UCLA), Pattie Maes (MIT Media Lab), Pat Pataranutaporn (MIT Media Lab)

**Models Tested**: Claude 4 (Anthropic), ChatGPT-3.5 (OpenAI), Llama 4 (Meta), and Qwen 3 (Alibaba). The study also used Google's Nano Banana for age progression and ElevenLabs Voice Model v2 for voice cloning.

**Research Question**: How do different modalities (text chatbot, voice agent, photorealistic avatar) of AI-generated future selves influence psychological outcomes, particularly Future Self-Continuity, emotional well-being, and motivation? Which large language model performs best for such interventions?

**Claim**: All personalized AI-generated future-self modalities significantly strengthen Future Self-Continuity, hope, and motivation compared to control, with no significant differences between modalities after controlling for baseline scores. Claude 4 outperforms competing LLMs by 40.8% across psychological and interaction dimensions, and interaction quality (particularly persuasiveness, realism, and engagement) predicts outcomes more strongly than modality choice.

**Method**: Randomized controlled trial (N=92) with four conditions: text-based chatbot, voice-only agent, photorealistic avatar with synchronized speech, and neutral voice control. Participants completed pre/post surveys measuring Future Self-Continuity (FSC), hope, emotional states, and interaction quality. Additionally, a systematic model comparison evaluated Claude 4, ChatGPT-3.5, Llama 4, and Qwen 3 using simulated participant data across 11 validated research metrics. Conversational content was analyzed using hierarchical clustering.

**Results**: All intervention conditions significantly improved Future Self-Continuity (voice B=0.76, avatar B=0.74, text B=0.60), hope (avatar +0.37, voice +0.37, text +0.33), and motivation compared to control, with no significant between-condition differences. Avatar showed largest gains in FSCQ Vividness (B=1.08) but only 0.22 difference from text. Claude 4 achieved 40.8% performance improvement over ChatGPT-3.5 across all metrics. Interaction quality metrics (persuasiveness r=0.39, realism r=0.35, engagement r=0.32) strongly predicted FSC gains. Content analysis revealed text focused on career (52.6%), while voice/avatar facilitated existential reflection (31.1%, 26.0%).

**Limitations**: Single-session design cannot assess long-term effects or sustained behavioral change. Sample size of 92 (23 per condition) may be underpowered for detecting subtle modality interactions or moderating effects of individual differences. Limited generalizability due to U.S.-based sample and cultural variations in self-concept and temporal orientation. Technical constraints included lag and desynchronization issues in avatar condition due to pre-generated lip movements and internet stability dependencies. The neutral control condition may not have provided an ideal baseline for isolating personalization versus presentation format effects.

---

*Generated with [Claude Code](https://claude.ai/code)*
