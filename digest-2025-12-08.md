# AI Evaluation Papers Digest - 2025-12-08

## Table of Contents
- [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](#understanding-llm-agent-behaviours-via-game-theory-strategy-recognition-biases-and-multi-agent-dynamics)
- [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](#an-ai-powered-autonomous-underwater-system-for-sea-exploration-and-scientific-research)

---

## [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](https://arxiv.org/abs/2512.07462v1)

**Authors & Affiliations**: Trung-Kiet Huynh, Duy-Minh Dao-Sy, and colleagues from Ho Chi Minh City University of Technology (HCMUT), Ho Chi Minh City University of Science (HCMUS), Vietnam National University - Ho Chi Minh City (VNU-HCM), Luxembourg Institute of Science and Technology, and Teesside University (UK). Corresponding authors are The Anh Han (Teesside University) and Le Hong Trang (HCMUT).

**Models Tested**: GPT-4o (OpenAI), Claude 3.5 Haiku (Anthropic), Mistral Large, and Llama 3.1 405B Instruct (Meta). Models were evaluated in English and Vietnamese across repeated game-theoretic scenarios.

**Research Question**: How do LLMs behave strategically in repeated social dilemmas, and what underlying intentions guide their cooperative versus defective choices? Can we identify systematic biases across models, languages, and roles using game-theoretic environments and machine learning strategy recognition?

**Claim**: LLMs exhibit systematic, model- and language-dependent behavioral patterns in strategic settings that can be characterized through game-theoretic analysis. Linguistic framing effects can be as strong as architectural differences, with models showing distinct strategic signatures (e.g., Claude is more cooperative, GPT-4o is highly language-sensitive, Mistral shows balanced strategy distribution).

**Method**: The authors extended the FAIRGAME framework with two game-theoretic environments: (1) a payoff-scaled Prisoner's Dilemma testing incentive sensitivity across three scaling factors (Î» = 0.1, 1.0, 10.0), and (2) a three-player Public Goods Game with dynamic payoffs and configurable personalities. They trained LSTM and other classifiers on canonical strategies (ALLC, ALLD, TFT, WSLS) to infer behavioral intentions from gameplay trajectories. Experiments included 10-round games with multiple independent runs per condition.

**Results**: LLMs show incentive-sensitive cooperation that increases with multiplication factors. Claude maintains prosocial bias even under selfish framing, GPT-4o exhibits extreme linguistic sensitivity with perfect personality adherence, and Mistral demonstrates language-invariant stability. Arabic and Vietnamese prompts elicit more defection, while English and Chinese favor adaptive strategies. Cross-linguistic cooperation gaps reach 29 percentage points. Agent position (first vs. second mover) significantly affects strategy choice, with first-mentioned agents defaulting to defection more frequently.

**Limitations**: The 10-round game horizon may be too short to capture sophisticated long-term strategies like reputation-building or forgiveness. Linguistic coverage is limited to English and Vietnamese, preventing broader cross-cultural conclusions. The study uses only two game-theoretic settings with fixed parameters (e.g., three-player groups in PGG). Strategy recognition focuses on four canonical strategies, potentially missing emergent hybrid behaviors. No parallel human experiments were conducted to assess ecological validity. High-confidence filtering (p>0.9) excludes potentially interesting ambiguous cases that may represent novel LLM strategies.
## [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652v1)

**Authors & Affiliations**: Hamad Almazrouei, Mariam Al Nasseri, and Maha Alzaabi are all affiliated with the College of Technological Innovation at Zayed University in Abu Dhabi, United Arab Emirates.

**Models Tested**: GPT-4o Mini (OpenAI's Large Language Model), YOLOv12 Nano for object detection, and ResNet50 (CNN) for feature extraction. The paper specifically evaluates GPT-4o Mini for generating structured reports and summaries of underwater findings.

**Research Question**: How can an AI-powered autonomous underwater vehicle system automate underwater object detection, analysis, and reporting to overcome challenges of extreme conditions, limited visibility, and high costs in sea exploration?

**Claim**: An integrated system combining YOLOv12 Nano, ResNet50, PCA, K-Means++ clustering, and GPT-4o Mini can effectively automate underwater exploration by detecting marine objects, clustering them based on visual characteristics, and generating insightful natural language summaries of findings.

**Method**: The system uses YOLOv12 Nano for real-time object detection on 55,000+ underwater images, ResNet50 for feature extraction, PCA for dimensionality reduction (preserving 98% variance), K-Means++ for clustering similar objects, and GPT-4o Mini for generating structured reports from detections and cluster data with location coordinates.

**Results**: YOLO achieved mAP@0.5 of 0.512, precision of 0.535, and recall of 0.438. PCA reduced features to 199 components while preserving 98% variance. K-Means successfully grouped 687 detections into 27 clusters. GPT-4o Mini effectively generated summaries describing object characteristics, environment, and common locations, though faced challenges with token-intensive HTML cluster visualization files.

**Limitations**: The paper faces significant limitations including class imbalance in the training dataset affecting YOLO performance, hardware constraints limiting PCA processing of large detection sets (>2,000 crops), moderate detection metrics suggesting room for improvement, challenges with LLM integration for cluster visualization due to token limits, and dataset quality issues (low lighting, blur, color distortion). The system was only tested on Australian marine environments and required lowering confidence thresholds to maintain detection under varying lighting conditions.

---

*Generated with [Claude Code](https://claude.ai/code)*
