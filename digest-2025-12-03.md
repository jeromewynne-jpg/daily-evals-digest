# AI Evaluation Papers Digest - 2025-12-03

## Table of Contents
- [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](#balancing-safety-and-helpfulness-in-healthcare-ai-assistants-through-iterative-preference-alignment)
- [ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models](#responsiblerobotbench-benchmarking-responsible-robot-manipulation-using-multi-modal-large-language-models)

---

## [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210v1)

**Authors & Affiliations**: Huy Nghiem (University of Maryland), Swetasudha Panda (Oracle Labs), Devashish Khatwani (Oracle Health AI), Huy V. Nguyen (Oracle Health AI), Krishnaram Kenthapadi (Oracle Health AI), Hal Daumé III (University of Maryland)

**Models Tested**: Meta Llama-3.2-3B-Instruct, Meta Llama-3.1-8B-Instruct, Meditron-8B (fine-tuned from Llama-3.1-8B), Mistral-7B-Instruct-v0.3. GPT-4o-mini from OpenAI is used as a judge model for evaluation.

**Research Question**: How can we balance safety and helpfulness in healthcare AI assistants through post-deployment iterative alignment, avoiding both unsafe compliance with harmful requests and over-refusal of benign queries?

**Claim**: An iterative post-deployment alignment framework using KTO and DPO can significantly improve safety metrics (up to 42%) in healthcare LLMs while managing the safety-helpfulness trade-off, with effectiveness varying by model architecture and judge reliability.

**Method**: The authors apply iterative cycles of Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) using the CARES-18K benchmark dataset. They evaluate using self-evaluation, base external judges (Llama-3B), and fine-tuned external judges, generating responses and safety judgments, then fine-tuning models with LoRA adapters across multiple cycles.

**Results**: Models show substantial safety improvements (up to 42% in Safety Score for Meditron-8B). Architecture-dependent calibration biases were observed: Llama-3B shows high self-evaluation reliability (Cohen's κ=0.59), while Llama-8B, Meditron-8B, and Mistral-7B show lower reliability (κ=0.29-0.37). External judges reduce bias amplification but introduce their own calibration caveats. All aligned models outperform baselines and generally surpass GPT-4o-mini on test metrics.

**Limitations**: The study relies exclusively on the synthetic CARES dataset under U.S.-centric assumptions, limiting generalizability to other domains, languages, or regulatory contexts. Limited clinical validation with human experts; GPT-4o-mini serves as proxy judge but isn't fully validated. Focus on small-to-mid-sized models (3-8B parameters); larger models and ensemble approaches not explored. No real-world deployment testing or longitudinal assessment of model drift. Stopping criteria and optimal α weight selection require further validation across diverse use cases.
## [ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models](https://arxiv.org/abs/2512.04308v1)

**Authors & Affiliations**: Lei Zhang (University of Hamburg, Agile Robots SE), Ju Dong (Technical University of Munich, Agile Robots SE), Kaixin Bai (University of Hamburg, Agile Robots SE), Minheng Ni (Hong Kong Polytechnic University), Zoltan-Csaba Márton (Agile Robots SE), Zhaopeng Chen (Agile Robots SE), Jianwei Zhang (University of Hamburg)

**Models Tested**: GPT-4o, GPT-4o mini (OpenAI), Qwen 7B and 72B (Alibaba), InternVL 2.5 4B. The paper evaluates these models across 23 multi-stage robotic manipulation tasks with varying hazard types (electrical, fire/chemical, human-related) and complexity levels.

**Research Question**: How can we systematically evaluate the ability of large multimodal models to perform safe and responsible robotic manipulation in high-stakes, hazardous environments? Can these models detect risks, reason about safety constraints, and execute physically grounded plans while maintaining ethical alignment?

**Claim**: ResponsibleRobotBench provides a comprehensive benchmark for evaluating LMM-driven responsible robotic manipulation. Current models show significant limitations in spatial planning, risk perception, and safe action execution, particularly in long-horizon tasks and scenarios requiring complex motion planning, despite strong semantic understanding capabilities.

**Method**: The authors created 23 multi-stage manipulation tasks in simulation with diverse hazard types and scene configurations. They developed a modular evaluation framework supporting multiple action representations (high-level skills, manipulation poses, code generation), in-context learning with N-shot examples, cognitive information prompting, and human-in-the-loop capabilities. Evaluation metrics include success rate, safety rate, safe success rate, cost, and hazard detection accuracy across 100 scene variants per task.

**Results**: GPT-4o achieved the highest overall performance (72% safety rate, 82% success rate, 64% safe success rate). Performance degraded significantly with increased planning complexity and long-horizon tasks (5+ sub-tasks). Human-related hazards showed highest detection accuracy while fire/chemical hazards showed lowest safety success rates. Models demonstrated poor spatial reasoning when directly predicting manipulation poses (near-zero success rates). Human-in-the-loop significantly improved safety compared to autonomous execution.

**Limitations**: The evaluation is conducted entirely in simulation (CoppeliaSim), which does not capture all nuances of real-world contact dynamics or unstructured human-robot interactions. The benchmark currently has limited task diversity (23 tasks) and may not cover all safety-critical scenarios. The evaluation framework requires pre-defined skills and structured environments, limiting generalizability. GPT-4o showed over-conservative refusal behavior after June 2025 updates, making it unsuitable for some evaluations. The cost metric is empirically set rather than derived from actual deployment data.

---

*Generated with [Claude Code](https://claude.ai/code)*
