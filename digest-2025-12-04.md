# AI Evaluation Papers Digest - 2025-12-04

## Table of Contents
- [Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems](#chameleon-adaptive-adversarial-agents-for-scaling-based-visual-prompt-injection-in-multimodal-ai-systems)
- [David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?](#david-vs-goliath-can-small-models-win-big-with-agentic-ai-in-hardware-design)
- [Towards Cross-View Point Correspondence in Vision-Language Models](#towards-cross-view-point-correspondence-in-vision-language-models)
- [DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](#datagovbench-benchmarking-llm-agents-for-real-world-data-governance-workflows)
- [Multi-LLM Collaboration for Medication Recommendation](#multi-llm-collaboration-for-medication-recommendation)
- [LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](#lexgenius-an-expert-level-benchmark-for-large-language-models-in-legal-general-intelligence)
- [AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages](#adibhashaa-a-community-curated-benchmark-for-machine-translation-into-indian-tribal-languages)
- [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](#towards-a-cultural-intelligence-and-values-inferences-quality-benchmark-for-community-values-and-common-knowledge)
- [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](#decoding-the-black-box-discerning-ai-rhetorics-about-and-through-poetic-prompting)

---

## [Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems](https://arxiv.org/abs/2512.04895v1)

**Authors & Affiliations**: M Zeeshan and Saud Satti, both from BSAI, FAST (of Aff.), Islamabad, Pakistan. Note: Both authors appear to share the same email address (i220615@nu.edu.pk), which may be an error.

**Models Tested**: Google Gemini-2.5-Flash model accessed via public API. The paper mentions evaluating against 'open-source VLMs' but specifically only tests Gemini-2.5-Flash.

**Research Question**: How can adversarial attackers exploit image scaling operations in VLM preprocessing pipelines to inject malicious visual prompts that are invisible before downscaling but become effective after processing? Can adaptive, feedback-driven optimization significantly improve attack success rates compared to static approaches?

**Claim**: The Chameleon framework, using iterative agent-based optimization with real-time VLM feedback, can reliably exploit scaling vulnerabilities in production VLMs to inject hidden malicious prompts, achieving 84.5% attack success rate (compared to 32.1% for static attacks) and compromising multi-agent decision-making systems.

**Method**: An adaptive adversarial framework that iteratively refines image perturbations using feedback from the target VLM. Two optimization strategies are tested: hill-climbing (greedy local search) and genetic algorithm (population-based search). High-resolution images (4368×4368) are perturbed and evaluated after downsampling. A reward function balances attack success, visual imperceptibility, and model confidence. Experiments test across different prompts, interpolation methods, and measure ASR, visual distance (L2), convergence iterations, and API efficiency.

**Results**: Chameleon achieved 87-91% attack success rate across optimization strategies, significantly outperforming the 32.1% baseline. Genetic algorithm slightly outperformed hill-climbing (91% vs 87%) but required more API calls (15.84 vs 12.47 per trial). Perturbations remained imperceptible (normalized L2 distance < 0.1). Attacks reduced VLM confidence by 0.18-0.21 and generalized across different prompts (84-93% success) and downsampling methods (86-92% success). Decision-making accuracy in multi-step tasks degraded by over 45%.

**Limitations**: The evaluation focused only on a single model (Gemini-2.5-Flash), limiting generalizability claims. The dataset was small (20 images) and may not capture edge cases or out-of-distribution content. No actual defense mechanisms were tested—only proposed theoretically. The free-tier API constraints may have limited experimental scope. The authors acknowledge cross-model transferability and defense evaluation as gaps requiring future work. The paper lacks comparison with other adaptive attack frameworks beyond static baselines.
## [David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?](https://arxiv.org/abs/2512.05073v1)

**Authors & Affiliations**: Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Sukanta Bhattacharjee, Chandan Karfa (Indian Institute of Technology, Guwahati, India); Animesh Basak Chowdhury (NXP USA, Inc.)

**Models Tested**: Small Language Models: SmolLM2 (1.7B), Nemotron-Mini (4B), Granite-4 (3B), DeepSeek-R1 (7B), GPT-OSS (20B), Phi-3.5-mini-instruct. Large Language Models: GPT-4o-mini (OpenAI). Also references Qwen-Coder (Alibaba).

**Research Question**: Can small language models (SLMs) with <20B parameters achieve near-LLM performance on hardware design tasks when equipped with sophisticated agentic AI frameworks, offering better cost-efficiency trade-offs?

**Claim**: Small language models paired with well-designed agentic frameworks can match or exceed LLM performance on specific hardware design tasks at significantly lower computational cost and energy consumption, demonstrating that 'strategy over scale' is viable for AI-assisted hardware design.

**Method**: The authors developed a five-agent framework (Planning & Pre-processing, SLM-aware Prompt Engineering, CodeGen, Validation, and Adaptive Feedback agents) and evaluated SLMs against LLMs on NVIDIA's CVDP benchmark across code generation and comprehension tasks, comparing single-shot vs. agentic performance with up to 5 iterative refinement rounds.

**Results**: On code generation task cid007, DeepSeek-R1 and Granite-4 with agentic framework achieved 51.25% and 48.75% pass rates respectively, surpassing GPT-4o-mini's 44.74%. For code comprehension, SLMs like phi-3.5-mini-instruct and DeepSeek-R1 matched or exceeded LLM performance (82-92% on cid009/cid010). Agentic frameworks produced 30-140% relative improvement for SLMs across code generation tasks.

**Limitations**: The study is limited to NVIDIA's CVDP benchmark and excludes the most complex 'Agentic Code Generation' category from evaluation. Several SLMs showed anomalies on specific tasks (e.g., Nemotron and Granite-4 on cid002, cid003, cid016). The framework requires up to 5 iterative rounds which may offset computational savings. The paper does not provide detailed cost comparisons or energy measurements for the full agentic pipeline. Code comprehension tasks use LLM-as-judge (GPT-5-mini) which may introduce bias.
## [Towards Cross-View Point Correspondence in Vision-Language Models](https://arxiv.org/abs/2512.04686v2)

**Authors & Affiliations**: Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng from Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beihang University, Jilin University, National University of Singapore, Peking University, Beijing Academy of Artificial Intelligence, and Huazhong University of Science and Technology

**Models Tested**: Gemini-2.5-Pro, Claude-Sonnet-4, o3, Qwen2.5-VL (3B, 7B, 32B, 72B), Qwen3-VL (30B, 235B), GPT-4o, RoboPoint, RoboRefer, RoboBrain-2.0, Molmo-7B-D, and the proposed CroPond (3B, 7B) models

**Research Question**: How can Vision-Language Models achieve precise point-level cross-view correspondence, which is crucial for spatial understanding and embodied AI applications like robotic manipulation?

**Claim**: Current state-of-the-art VLMs significantly lag behind human performance in cross-view point correspondence tasks (54.65% gap), but performance can be substantially improved through targeted training on affordance-focused cross-view data, with CroPond surpassing Gemini-2.5-Pro by 39.7% accuracy

**Method**: The authors introduce CrossPoint-Bench, a hierarchical benchmark with four task types (Fine-grained Grounding, Visibility Reasoning, Correspondence-Judgement, Correspondence-Pointing), construct CrossPoint-378K dataset with 378K QA pairs across 900 indoor scenes focused on affordance regions, and train CroPond models using supervised fine-tuning on multi-source data including CrossPoint-378K, spatial understanding datasets, and instruction-tuning data

**Results**: Gemini-2.5-Pro achieves only 37.1% overall accuracy compared to human 91.75%, with particularly poor performance on point-level prediction (16.41% vs 93.63%). CroPond-7B achieves 76.8% overall accuracy, outperforming all baselines. Performance degrades sharply from discrete choice tasks (49.8%) to continuous coordinate prediction (12.78%) for general VLMs. Models perform worse on semantic parts (36.47%) than general objects (46.48%). CroPond also generalizes well to spatial understanding benchmarks while maintaining general VLM capabilities

**Limitations**: CroPond uses only simple supervised fine-tuning without advanced strategies like reinforcement learning. The evaluation is limited to indoor scenes with relatively static environments. The work focuses primarily on perception rather than integrating CVPC with multi-agent task planning. Error analysis reveals persistent issues with frame transfer failure, spatial reconstruction failure, and semantic-point decoupling that are not fully resolved. The dataset is constructed using automated pipelines which may introduce biases or errors despite manual verification steps
## [DataGovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416v2)

**Authors & Affiliations**: Zhou Liu, Zhaoyang Han, Guochen Yan, Hao Liang, Bohan Zeng, Wentao Zhang (Peking University, China); Xing Chen, Yuanfeng Song (ByteDance, China)

**Models Tested**: GPT-5, GPT-4o, o1, o3, o4-mini, Claude-4-sonnet, Claude-4-opus, Gemini-2.5-flash, Grok-3, Grok-4, Kimi-K2-instruct, DeepSeek-V3, Qwen3-235b-a22b, Qwen2.5-coder, Qwen3-coder, Llama-3-70B, Llama-4-scout, Mistral-7B, Gemma-3-27B, Phi4

**Research Question**: The paper addresses how to evaluate and improve LLM agents' capabilities in automating data governance workflows, specifically their ability to translate natural language instructions into correct data transformation code that ensures data quality and correctness.

**Claim**: DataGovBench provides the first comprehensive benchmark for data governance automation with 150 real-world tasks using a reversed-objective methodology for noise synthesis. DataGovAgent, with its Planner-Executor-Evaluator architecture using contract-guided planning and feedback-driven debugging, significantly outperforms existing approaches on complex data governance tasks.

**Method**: The authors created a hierarchical benchmark (operator-level and DAG-level tasks) across 6 data governance scenarios using real-world data from Statista. They developed a reversed-objective methodology to synthesize realistic noise and task-specific evaluation scripts. DataGovAgent employs contract-based planning, retrieval-augmented generation from a curated operator library, and sandboxed iterative debugging with structured feedback.

**Results**: DataGovAgent with GPT-5 achieves 64% TSR on operator-level tasks (+15pp over baseline) and 60% on DAG-level tasks (+14pp). It reduces average debug iterations from 14.89 to 3.29 compared to ChatDev baseline. Even top models like GPT-5 struggle with <50% TSR in single-round generation, and there's a significant gap between code runnability (CRR) and correctness (TSR), demonstrating the benchmark's difficulty.

**Limitations**: The benchmark construction requires intensive manual curation, limiting scalability for training-scale datasets. The iterative debugging approach incurs higher token consumption and latency compared to single-pass models. Governance contracts may not fully capture semantic nuances of highly ambiguous instructions, potentially generating structurally correct but semantically misaligned solutions. The paper acknowledges these efficiency-quality trade-offs but provides limited analysis of when simpler approaches might suffice.
## [Multi-LLM Collaboration for Medication Recommendation](https://arxiv.org/abs/2512.05066v1)

**Authors & Affiliations**: Huascar Sanchez and Briland Hitaj (Computer Science Laboratory, SRI International), Jules Bergmann (SRI International and University of Maryland St. Joseph Medical Center), Linda Briesemeister (SRI International)

**Models Tested**: OpenAI models (GPT-4o, GPT-5, o1-mini, o3-mini, o4-mini), Anthropic Claude models (claude-3-7-sonnet-20250219, claude-opus-4-1, claude-sonnet-4-5), Google Gemini models (gemini-2.0-flash, gemini-2.5-flash), and open-source models (firefunction-v2, gpt-oss:20b, qwen3:32b, qwen2.5:32b)

**Research Question**: Can Chemistry-based multi-LLM collaboration ensure efficient, effective, stable, and calibrated collaboration for medication recommendation from clinical notes? Which LLM sampling strategy yields the most accurate results?

**Claim**: LLM Chemistry-guided multi-LLM collaboration improves medication recommendation reliability by explicitly modeling synergistic and antagonistic relationships among models, achieving better efficiency, effectiveness, stability, and calibration compared to naive ensemble approaches (LOCAL, REMOTE, RANDOM sampling).

**Method**: The authors implement a two-stage framework (generation and evaluation) where N=3 LLMs generate medication recommendations from clinical vignettes, then evaluate each other's outputs. They compare four sampling strategies (LOCAL, REMOTE, RANDOM, CHEMISTRY) using 20 synthetic clinical vignettes validated by domain experts. The CHEMISTRY strategy uses their prior work's compatibility metric to select optimal LLM ensembles.

**Results**: The CHEMISTRY ensemble (three Claude models) achieved 11-second average generation time (49x faster than LOCAL, 9x faster than RANDOM/REMOTE), 0.78 accuracy (comparable to REMOTE's 0.84), lowest variance in agreement (0.05 vs 0.11 for REMOTE/RANDOM and 1.05 for LOCAL), and demonstrated superior stability with no execution failures unlike REMOTE strategy.

**Limitations**: The study uses only 20 synthetic clinical vignettes validated by domain experts rather than real patient data. Zero-shot prompting was used without retrieval-augmented generation capabilities. Limited patient context was provided (brief notes without medications, dosages, allergies). Small sample size and synthetic data limit generalizability to real clinical settings. No comparison with clinical practice guidelines or assessment of clinical safety.
## [LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://arxiv.org/abs/2512.04578v1)

**Authors & Affiliations**: Wenjin Liu, Xin Feng, Xiang Ji, Lijuan Zhou (Hainan University); Haoran Luo, Rui Mao, Erik Cambria (Nanyang Technological University); Jiapu Wang (Nanjing University of Science and Technology); Shirui Pan (Griffith University)

**Models Tested**: DeepSeek-LLM-7B-Chat, DeepSeek-R1, DeepSeek-V3, Qwen-2.5-7B-Instruct, Qwen-2.5-1.5B-Instruct, Qwen-3-8B, Qwen-3-4B, GLM-4-9B-Chat, LLaMA-3.2-1B-Instruct, LLaMA-3.2-8B-Instruct, GPT-4o mini, GPT-4.1 nano

**Research Question**: How can we systematically evaluate legal general intelligence in LLMs across multiple cognitive dimensions, tasks, and abilities, particularly in the Chinese legal domain? Existing benchmarks are result-oriented and fail to systematically assess legal intelligence capabilities.

**Claim**: LexGenius provides the first expert-level, comprehensive benchmark for evaluating legal general intelligence in LLMs through a three-level Dimension-Task-Ability framework covering 7 dimensions, 11 tasks, and 20 abilities. Even the best-performing LLMs (DeepSeek-R1) show significant gaps compared to human legal experts across all dimensions.

**Method**: The authors developed 8,385 multiple-choice questions from recent Chinese legal cases and exam questions, reviewed by legal professionals. They structured evaluation across three hierarchical levels: dimensions (legal understanding, reasoning, application, ethics, language, law & society, judicial practice), tasks (11 scenario-based tasks), and abilities (20 atomic legal intelligence abilities). Models were tested using both naive and chain-of-thought prompting.

**Results**: DeepSeek-R1 achieved the highest performance (64.45% average) but still significantly lagged behind human experts (87.64% average). LLMs performed relatively well on static knowledge tasks (legal provisions understanding) but poorly on tasks requiring dynamic reasoning, ethical judgment, and value trade-offs. Legal soft intelligence (ethical judgment, law-morality boundaries) showed systematic weaknesses across all models. CoT prompting showed limited or negative effects on legal tasks.

**Limitations**: The benchmark is limited to Chinese law and unimodal (text-only) evaluation, excluding multimodal legal evidence common in real cases. It lacks temporal dimension testing to evaluate understanding of law changes over time. The benchmark is constrained to a single legal system (Chinese civil law), limiting cross-jurisdictional generalizability. Enhanced methods like CoT showed negative transfer in some cases, suggesting the evaluation framework may not fully capture reasoning processes.
## [AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages](https://arxiv.org/abs/2512.04765v1)

**Authors & Affiliations**: Pooja Singh (Department of Electrical Engineering, Indian Institute of Technology Delhi), Sandeep Kumar (Department of Electrical Engineering and Yardi School of Artificial Intelligence, Indian Institute of Technology Delhi)

**Models Tested**: NLLB-200, mT5, IndicTrans2, Bloom, Gemini 2.5 Flash, GPT-4o-mini were evaluated for machine translation into four Indian tribal languages (Bhili, Mundari, Gondi, and Santali)

**Research Question**: How can machine translation systems be developed for severely under-resourced Indian tribal languages that have minimal representation in existing NLP systems and datasets?

**Claim**: Community-driven participatory corpus creation combined with fine-tuning of multilingual models can substantially improve machine translation support for under-resourced tribal languages, while also establishing a methodological template for equitable AI research.

**Method**: The researchers created an 80,000 sentence parallel corpus (20,000 per language) through a three-stage participatory workflow: source curation from Hindi texts, community translation by native speakers, and independent validation. They then fine-tuned encoder-decoder MT models and evaluated them against zero-shot and few-shot LLMs using chrF++, BLEU, and human evaluation.

**Results**: Fine-tuned multilingual MT models consistently outperformed zero-shot baselines. Translation into high-resource languages (tribal→Hindi/English) systematically outperformed translation into tribal languages. Few-shot LLMs provided competitive baselines but generally underperformed dedicated fine-tuned MT models, especially for Santali in Ol Chiki script.

**Limitations**: The corpus size is modest (20,000 sentence pairs per language). The paper lacks detailed quantitative results and specific performance metrics. English translations were produced by another MT model (IndicTrans2) and only sample-validated rather than fully human-verified. The asymmetry in translation quality suggests insufficient coverage of morphologically rich tribal language vocabulary and domains.
## [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176v1)

**Authors & Affiliations**: Brittany Johnson (George Mason University), Erin Reddick (ChatBlackGPT), Angela D.R. Smith (University of Texas at Austin)

**Models Tested**: The paper proposes future evaluation of ChatGPT (general purpose) and ChatBlackGPT (culturally-informed LLM). This is a research agenda paper describing planned work, not completed evaluations. The authors reference evaluating 'seven LLMs' in KorNAT's prior work and plan to 'experiment with top LLMs, both general-purpose and culturally informed' using their benchmark.

**Research Question**: How can we develop and evaluate LLMs for cultural alignment with specific communities, particularly the Black community in the U.S., rather than attempting broad national or general population alignment?

**Claim**: The paper proposes CIVIQ (Cultural Intelligence and Values Inference Quality), a community-centric benchmark for evaluating LLM alignment with Black culture in the U.S., adapting the KorNAT methodology to focus on community-level rather than national-level cultural alignment. This benchmark will enable assessment of both general-purpose LLMs (like ChatGPT) and culturally-informed LLMs (like ChatBlackGPT).

**Method**: The authors propose adapting KorNAT's methodology: (1) curating social values topics from Black media outlets and historical conflict areas, (2) generating ~4,000 Likert-scale survey items using LLMs with human revision, (3) developing ~6,000 multiple-choice common knowledge items with Black historians and educators, (4) conducting stratified sampling of 6,000+ Black American adults across demographic dimensions, (5) calculating Social Value Alignment (SVA) and Common Knowledge Alignment (CKA) metrics based on survey responses.

**Results**: This is a research agenda paper proposing future work. No empirical results are presented yet. The authors outline anticipated outcomes including a leaderboard, sampled item set for public benchmarking, and controlled full dataset access under a community data covenant.

**Limitations**: As this is a proposal paper, key limitations include: (1) No completed implementation or validation of the proposed benchmark, (2) Reliance on LLM-generated survey items which may introduce biases, (3) Challenges in recruiting diverse respondents from historically marginalized communities, (4) Potential for community pushback regarding broad access to cultural values data, (5) Difficulty ensuring the benchmark captures the heterogeneity within Black communities across diaspora, region, socioeconomic status, and other dimensions, (6) The stratified sampling approach may still miss important subgroups or perspectives.
## [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243v1)

**Authors & Affiliations**: P.D. Edgar (Texts & Technology, University of Central Florida, Orlando, USA); Alia Hall (Texts & Technology, University of Central Florida, Orlando, USA)

**Models Tested**: ChatGPT (OpenAI), Claude (Anthropic), and DeepSeek were evaluated through approximately 30 prompts about poet Maya Angelou and her works (Alone, On the Pulse of Morning, Still I Rise)

**Research Question**: What rhetorical tropes and narratives do LLMs iterate when discussing poets and their work, and how far will these models go in appropriating previously written creative works when asked to adapt them for different audiences?

**Claim**: Creative text prompting, specifically 'Poetry Prompt Patterns,' can serve as a diagnostic tool to reveal algorithmic biases, tendencies toward stereotyping, and problematic cultural appropriation in LLMs, particularly around issues of race and identity when discussing Black feminist writers like Maya Angelou.

**Method**: The authors developed a 'Poetry Prompt Pattern' involving investigation, analysis, composition, adaptation, and explanation prompts. They conducted exploratory interactions with three LLMs using a scaffolded script about Maya Angelou's poetry, then analyzed outputs using Critical Discourse Analysis (CDA), word frequency analysis, and word clouds to identify rhetorical patterns, stereotypes, and appropriation tendencies.

**Results**: All models produced panegyric descriptions emphasizing 'resilience,' 'empowerment,' and 'universal themes' while potentially muting political complexity. ChatGPT readily rewrote all poems; Claude resisted rewriting 'Still I Rise' on ethical grounds but complied when asked for a 'companion poem'; DeepSeek showed technical resistance initially. Models conflated 'universality' with less-specific language and 'audience' with geographic markers, revealing tensions in how they handle cultural specificity versus accessibility.

**Limitations**: The study focused on a single poet (Maya Angelou) and three poems, limiting generalizability. The methodology is exploratory and qualitative without quantitative validation. The authors acknowledge that poetic contexts may lead to hallucinations and that their prompt variations across models reduce reproducibility. The research design involved iterative, flexible prompting rather than standardized testing, making systematic comparison difficult. No subject matter experts were consulted to validate interpretations of bias in the poetry analysis.

---

*Generated with [Claude Code](https://claude.ai/code)*
