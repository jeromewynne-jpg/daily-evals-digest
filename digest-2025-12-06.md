# AI Evaluation Papers Digest - 2025-12-06

## Table of Contents
- [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](#are-ai-generated-driving-videos-ready-for-autonomous-driving-a-diagnostic-evaluation-framework)
- [Small Language Models Can Use Nuanced Reasoning For Health Science Research Classification: A Microbial-Oncogenesis Case Study](#small-language-models-can-use-nuanced-reasoning-for-health-science-research-classification-a-microbial-oncogenesis-case-study)

---

## [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376v1)

**Authors & Affiliations**: Xinhao Xiang (IFM Lab, University of California, Davis), Abhijeet Rastogi (IFM Lab, University of California, Davis), Jiawei Zhang (IFM Lab, University of California, Davis)

**Models Tested**: The paper evaluates AI-generated driving videos from major video generation models: Veo 3 (Google), Sora (OpenAI), Pika, and HunyuanVideo (Tencent). The ADGVE evaluator uses Qwen2-VL as the vision-language model for quality assessment. Downstream perception models tested include DETR, YOLOv8, ByteTrack, MOTR, Mask2Former, and SegFormer.

**Research Question**: Can AI-generated driving videos from prompt-only text-to-video models reliably support training and evaluation of autonomous driving perception models? What quality issues exist and how can they be systematically detected and filtered?

**Claim**: Raw AI-generated driving videos contain systematic failure modes (temporal instability, physical inaccuracy, traffic rule violations) that harm downstream perception tasks. The proposed ADGVE evaluator can effectively filter these videos, turning them from harmful noise into beneficial training augmentation when combined with real-world data.

**Method**: The authors develop a taxonomy of six failure modes for AI-generated driving videos, create ADGV-Bench (a benchmark with 90 videos, dense annotations, and human quality labels), and propose ADGVE—a multi-component evaluator combining static perception checks, temporal consistency analysis, lane-obedience scoring, and VLM-guided reasoning. They test filtering effectiveness by training perception models (detection, tracking, segmentation) on raw vs. filtered videos.

**Results**: Blindly adding raw AI-generated videos degrades perception performance on KITTI (AP drops 2.6 points for DETR). ADGVE filtering consistently improves all tested metrics: in-domain AP gains of +9.7 for DETR, +22.3 MOTA for MOTR, and cross-domain improvements over KITTI-only baselines (+2.5 AP). ADGVE scores correlate well with existing VQA metrics and approximately double after filtering. The filtered subset approaches the quality of human-annotated high-confidence data.

**Limitations**: The benchmark is relatively small (90 videos, ~11K frames), limiting generalization claims. The lane obedience module uses hand-crafted rules with fixed weights rather than learned components. The study focuses only on front-view dashcam scenarios and does not evaluate planning/control tasks. The paper does not address the computational cost of ADGVE filtering at scale, and the threshold τ=0.2 is selected empirically on a dev set without systematic optimization. The evaluation is limited to three video generators and may not generalize to future models.
## [Small Language Models Can Use Nuanced Reasoning For Health Science Research Classification: A Microbial-Oncogenesis Case Study](https://arxiv.org/abs/2512.06502v1)

**Authors & Affiliations**: Muhammed Muaaz Dawood and Mohammad Zaid Moonsamy (University of the Witwatersrand, School of Computer Science and Applied Mathematics; also IDORI), Kaela Kokkas (University of the Witwatersrand, Department of Clinical Microbiology and Infectious Diseases; also IDORI), Hairong Wang and Richard Klein (University of the Witwatersrand, School of Computer Science and Applied Mathematics), Robert F. Breiman (IDORI, University of the Witwatersrand), Emmanuel K. Sekyi (OncoVectra, London, UK), Bruce A. Bassett (Wits MIND Institute and School of Computer Science and Applied Mathematics, University of the Witwatersrand; also IDORI)

**Models Tested**: Small Language Models: Llama 3 (8B), Qwen2.5 (7B), Qwen3 (8B), DeepSeek-R1 (7B), Mistral (7B), Meerkat (7B). Frontier LLMs: GPT-5 (API with low/high effort reasoning modes), GPT-5 Thinking Mini (UI), Gemini 2.5 Pro, Gemini 3 Pro Preview.

**Research Question**: Can Small Language Models (SLMs, ≤8B parameters) perform nuanced classification of biomedical research papers with accuracy comparable to frontier Large Language Models, and how do different prompting strategies affect their performance?

**Claim**: Small language models (≤8B parameters) can achieve competitive performance with frontier LLMs for nuanced biomedical literature classification when paired with simple prompting strategies. Llama 3 and Qwen2.5 outperform GPT-5 and match Gemini 2.5 Pro in binary classification with in-context learning, making them viable cost-efficient filters for AI co-scientist pipelines.

**Method**: The authors created an expert-validated dataset of 100 papers on HMTV/MMTV-like viruses and breast cancer, labeled as relevant, somewhat relevant, or irrelevant. They evaluated SLMs and frontier LLMs using zero-shot and few-shot (in-context learning) approaches, including AdalFlow and DSPy optimization frameworks. They tested various ICL strategies (random, fair, central, closest, and Bootstrap Few-Shot with Random Search) and conducted perturbation-based interpretability analysis by systematically removing noun phrases to understand decision-making.

**Results**: Llama 3 and Qwen2.5 outperformed GPT-5 (both effort levels), Gemini 3 Pro Preview, and Meerkat in zero-shot settings, though trailing Gemini 2.5 Pro. With in-context learning (ICL), particularly Fair sampling for Llama 3 and BFS-RS for Qwen2.5, the SLMs matched Gemini 2.5 Pro's performance in binary classification (accuracy 0.91-0.92, F1 0.89). GPT-5 exhibited overly strict causal interpretation, systematically underclassifying relevant papers. The SOMEWHAT RELEVANT class proved most challenging for all models. Perturbation analysis revealed SLMs rely on valid scientific cues but can be influenced by spurious textual artifacts.

**Limitations**: The dataset is small (100 papers) and limited to a single microbe-cancer pair, potentially limiting generalizability. Expert labeling is time-intensive and involves some inherent ambiguity at class boundaries. The study only evaluated titles and abstracts, not full papers. Only two SLMs (Llama 3 and Qwen2.5) received extensive testing due to computational constraints, limiting model coverage. BFS-RS optimization required 100-200 hours per model with single-threaded execution. The study does not validate the full AI co-scientist pipeline end-to-end, focusing only on the filtering stage. Performance may vary across different microbe-cancer pairs depending on representation in pre-training data.

---

*Generated with [Claude Code](https://claude.ai/code)*
